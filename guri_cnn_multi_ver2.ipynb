{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 원본스케일 but 개수 줄여서 테스팅\n",
    "### 일단 배치 사이즈 늘리는게 최우선이긴함 \n",
    "### 빠르게 보고하고 새로 학습할때는 자원이 얼마나 필요할지 계산을 좀 해보자\n",
    "### 배치 사이즈 왜 못늘리는지 다시 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "list_coords = os.listdir(f'/raid/wlqor98/patch_info/coords/patch{224}_{64000}_level{0}')\n",
    "#print(len(list_coords))\n",
    "for path in list_coords:\n",
    "    pass\n",
    "    #print(len(os.listdir(f'./patch_info/coords/patch{224}_{64000}_level{0}/{path}') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 용으로 128개 씩 하는거 먼저 해보자 얼마나 걸리는지 보게\n",
      "다음 기기로 학습합니다: cuda\n",
      "dup img name : TCGA-T7-A92I\n",
      "dup img name : TCGA-AK-3427\n",
      "dup img name : TCGA-MM-A564\n",
      "dup img name : TCGA-A3-3365\n",
      "dup img name : TCGA-T7-A92I\n",
      "OSS : 0, DSS : 0 = 0.0\n",
      "OSS : 0, DSS : 1 = 108.0\n",
      "OSS : 1, DSS : 0 = 338.0\n",
      "OSS : 1, DSS : 1 = 0.0\n",
      "[[  0. 108.]\n",
      " [338.   0.]]\n"
     ]
    }
   ],
   "source": [
    "is_multi_gpu = False\n",
    "\n",
    "save_patch_samples = False\n",
    "show_val_slides = False\n",
    "\n",
    "show_accuracy = False\n",
    "show_slide_accuracy = False\n",
    "show_gradcam = False\n",
    "show_slide_pred_visualizer = False\n",
    "show_filter_analysis = False\n",
    "\n",
    "use_original_dataset = False # = !new_sampled_dataset(use_coord_dataset) # original : no coord data\n",
    "# use_npz = False #npz 차이 미미함\n",
    "\n",
    "if use_original_dataset:\n",
    "    if show_slide_pred_visualizer:\n",
    "        print('you cannot use slide pred visualizer')\n",
    "\n",
    "train = True\n",
    "learning_rate = 0.001\n",
    "training_epochs = 3# 이 이상부터 오버피팅 일어남 (7로 했을때)\n",
    "batch_size = 8 # but we will feed 4 data in getitem \n",
    "\n",
    "use_weight_decay = False\n",
    "weight_decay = 0.001 # 0.0001에서 0.001로 바꿈 (overfitting 너무 심해서)\n",
    "\n",
    "is_continous_training = False\n",
    "show_pbar = True\n",
    "\n",
    "# control dataset\n",
    "is_year_sort = True\n",
    "show_year_info = False # total year\n",
    "show_dataset_year_info = False\n",
    "\n",
    "\n",
    "use_val_model = True\n",
    "use_exist_model = False\n",
    "\n",
    "# control model (exist model)\n",
    "is_year_sort_model = True\n",
    "is_l2_norm_model = True\n",
    "\n",
    "is_cross_model_test = False # 위의 변수로 자동 할당 가능하다. 우선 보류\n",
    "if show_slide_pred_visualizer == False:\n",
    "    is_cross_model_test = False # 테스트를 교차검증 식으로 할건지에 대한 변수\n",
    "\n",
    "if use_exist_model:\n",
    "    #original sort : data불러오는 폴더의 indexing이 hash number 같은 느낌으로 되어있어서 어쨌든, 어떤 기준을 말하기는 애매함\n",
    "    # indexing 예시 : 0a2a3ea8-fe81-42e6-b313-e2d08d126dcb : TCGA-BP-4970 # 어쨌든 original sort는 이 순서대로 학습한 것임\n",
    "    model_name = None\n",
    "    if is_year_sort_model and is_l2_norm_model:\n",
    "        model_name = 'l2_year_sort'\n",
    "    elif is_year_sort_model and not is_l2_norm_model:\n",
    "        model_name = 'year_sort'\n",
    "    elif not is_year_sort_model and not is_l2_norm_model:\n",
    "        model_name = 'original_sort'\n",
    "    elif not is_year_sort_model and is_l2_norm_model:\n",
    "        model_name = 'l2_original_sort'\n",
    "\n",
    "    print(f'model name : {model_name}')\n",
    "\n",
    "\n",
    "# for original sort\n",
    "ratio_train=0.8\n",
    "ratio_val=0.1\n",
    "ratio_test=0.1\n",
    "\n",
    "level = 0\n",
    "patch_size = 224\n",
    "patches_num = 4000 * 4 * 4\n",
    "#patches_num = 128\n",
    "print('test 용으로 128개 씩 하는거 먼저 해보자 얼마나 걸리는지 보게')\n",
    "\n",
    "# coords와 slide를 통해서 sample img 불러오는 방법 사용\n",
    "patch_samples_root = f'/raid/wlqor98/patch_info/coords/patch{patch_size}_{patches_num}_level{level}'\n",
    "patch_coords_root = f'/raid/wlqor98/patch_info/coords/patch{patch_size}_{patches_num}_level{level}'\n",
    "img_root = f'/raid/wlqor98/Datasets/Guri_hospital/KIRC_TCGA_Dx_images_level2_downsample_whole/'\n",
    "svs_root = '/raid/Datasets/Guri_hospital/KIRC_TCGA_Dx_images/'\n",
    "\n",
    "slide_info_root = '/raid/wlqor98/Datasets/Pretrain/guri_test'\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')    # 경고 무시\n",
    "\n",
    "import openslide\n",
    "from multiprocessing import Pool\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # model!\n",
    "import torch.nn.functional as F # loss function!\n",
    "import torch.optim as optim # optimizer!\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import Dataset, DataLoader # 데이터로더\n",
    "import random\n",
    "import torch.nn.init\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import copy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"다음 기기로 학습합니다:\", device)\n",
    "\n",
    "random_seed = 1\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class DatasetMaker:\n",
    "    def __init__(self, is_temp_test=False, level=2, patch_size=64, patches_num=1024, patch_samples_root=None, img_root=None, svs_root=None):\n",
    "        self.img_root = img_root\n",
    "        self.xlsx_root = '/raid/Datasets/Guri_hospital/KIRC_TCGA_Dx_Clinical Data/2021-06-30_KIRC_clinical.xlsx'\n",
    "        self.clinic_data = None\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_samples_root = patch_samples_root\n",
    "        self.svs_root = svs_root\n",
    "    \n",
    "\n",
    "    def make_and_load_clinic_data_xlsx(self):\n",
    "        xlsx_root = self.xlsx_root\n",
    "\n",
    "        df = pd.read_excel(xlsx_root)\n",
    "        clinic_data = df[ ['PathologyReportFileName', 'DSS1', 'OverallSurvivalStatus', 'year_of_initial_pathologic_diagnosis'] ]\n",
    "        clinic_data.rename(columns = {'OverallSurvivalStatus' : 'OSS'}, inplace = True)\n",
    "\n",
    "        clinic_data.loc[clinic_data['OSS'] =='LIVING', 'OSS'] = 1\n",
    "        clinic_data.loc[clinic_data['OSS'] =='DECEASED', 'OSS'] = 0\n",
    "        clinic_data.dropna(inplace = True)\n",
    "        clinic_data.reset_index(inplace = True)\n",
    "\n",
    "        # for cutting pathology name length to 12\n",
    "        length = len(clinic_data['PathologyReportFileName'])\n",
    "        for i in range(length): clinic_data.loc[clinic_data['PathologyReportFileName'] == clinic_data['PathologyReportFileName'][i], 'PathologyReportFileName'] = clinic_data['PathologyReportFileName'][i][:12]\n",
    "        clinic_data.head()\n",
    "        clinic_data.to_excel('./clinic_data.xlsx')\n",
    "        self.clinic_data = clinic_data\n",
    "\n",
    "    def get_img_dictionary_and_name_list(self, is_DSS_0_used=False, is_label_ratio_same=False):\n",
    "        img_root = self.img_root\n",
    "        clinic_data = self.clinic_data\n",
    "\n",
    "        directory_candidates = os.listdir(img_root)\n",
    "        # d is for dictionary\n",
    "        # l is for list\n",
    "        d_img = {}\n",
    "\n",
    "        OSS_cnt = 0\n",
    "        DSS_cnt = 0\n",
    "        label_table = np.zeros((2,2))\n",
    "        for candidate in directory_candidates:\n",
    "            if len(candidate) == 36:\n",
    "                img_path = os.path.join(img_root, candidate)\n",
    "            if len(os.listdir(img_path)) == 0: continue\n",
    "            img = os.listdir(img_path)[0]\n",
    "            name = img[:12]\n",
    "\n",
    "            if name in d_img:\n",
    "                print(f'dup img name : {name}')\n",
    "                continue\n",
    "            \n",
    "            ## DSS or OSS label checker\n",
    "            label = clinic_data[clinic_data['PathologyReportFileName'] == name]\n",
    "            if len(label['OSS']) != 1 or len(label['DSS1']) != 1: continue\n",
    "            else : [OSS,DSS] = [list(label['OSS'])[0], list(label['DSS1'])[0]]\n",
    "            \n",
    "\n",
    "            if not is_DSS_0_used:\n",
    "                if DSS == 0 and OSS == 0: # 죽었는데 암으로 죽지는 않은 사람\n",
    "                    continue\n",
    "            \n",
    "                \n",
    "            if OSS == 1:\n",
    "                OSS_cnt +=1\n",
    "                if is_label_ratio_same:\n",
    "                    if OSS_cnt > 100:\n",
    "                        continue\n",
    "\n",
    "            if DSS == 1:\n",
    "                DSS_cnt +=1\n",
    "                if is_label_ratio_same:\n",
    "                    if DSS_cnt > 100:\n",
    "                        continue\n",
    "            \n",
    "            label_table[OSS][DSS] += 1\n",
    "            year_of_initial_pathologic_diagnosis = int(label['year_of_initial_pathologic_diagnosis'])\n",
    "            \n",
    "          \n",
    "            svs = img[:-3] + 'svs'\n",
    "            d_img[name] = {'img_root': os.path.join(img_path, img), 'OSS' : OSS, 'DSS' : DSS, \n",
    "            'svs_root' : os.path.join(self.svs_root, candidate, svs), 'year_of_initial_pathologic_diagnosis' : year_of_initial_pathologic_diagnosis}\n",
    "        \n",
    "        for OSS in [0,1]:\n",
    "            for DSS in [0,1]:\n",
    "                print(f\"OSS : {OSS}, DSS : {DSS} = {label_table[OSS][DSS]}\")\n",
    "        print(label_table)\n",
    "\n",
    "        l_name = sorted(list(set(d_img.keys())))\n",
    "        # print(l_name)\n",
    "\n",
    "        return d_img, l_name\n",
    "        \n",
    "    def load_image(self, target_dir, img_name, candidate):\n",
    "        img = np.load(f'{target_dir}/{img_name}/{candidate}')\n",
    "        return img\n",
    " \n",
    "        \n",
    "    def get_x_train_for_image(self, img_name):\n",
    "        target_dir = f'{self.patch_samples_root}/{img_name}'\n",
    "        candidates = os.listdir(target_dir)\n",
    "        print(len(candidates))\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            if candidate[-3:] == 'npy':\n",
    "                img = np.load(f'{target_dir}/{candidate}')\n",
    "                print(img.shape)\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "            if i>10:\n",
    "                break\n",
    "\n",
    "\n",
    "DM = DatasetMaker(is_temp_test=False, level=level, patch_size=patch_size, patches_num=patches_num, patch_samples_root=patch_samples_root, img_root=img_root, svs_root=svs_root)\n",
    "\n",
    "DM.make_and_load_clinic_data_xlsx()\n",
    "d_img, l_name = DM.get_img_dictionary_and_name_list(is_DSS_0_used=False, is_label_ratio_same=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "patch_list = os.listdir(patch_samples_root)\n",
    "print(len(patch_list))\n",
    "\n",
    "len(l_name)\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "def make_target_slide_list(root, d_img, purpose='train', slide_name=None, is_year_sort=False, show_year_info=False, show_dataset_year_info=False):\n",
    "  print(f'is_year_sort : {is_year_sort}')\n",
    "  folders = os.listdir(root)\n",
    "\n",
    "  ######### show year info of data\n",
    "  if purpose != 'slide' and purpose != 'gradcam':\n",
    "    year_list = []\n",
    "    for img_name in folders:\n",
    "      year = d_img[img_name]['year_of_initial_pathologic_diagnosis'] \n",
    "      year_list.append(year)\n",
    "    \n",
    "    # Pair each folder with its corresponding year\n",
    "    folder_year_pairs = zip(folders, year_list)\n",
    "\n",
    "    # Sort the pairs by year\n",
    "    sorted_pairs = sorted(folder_year_pairs, key=lambda x: x[1])\n",
    "\n",
    "    # Extract the sorted folders\n",
    "    # if we use monthly criteria, we will use sorted_folders\n",
    "    sorted_folders = [folder for folder, _ in sorted_pairs]\n",
    "\n",
    "    min_year = min(year_list)\n",
    "    max_year = max(year_list)\n",
    "    total_year_bins = range(min_year, max_year)\n",
    "\n",
    "    if show_year_info:\n",
    "      plt.hist(year_list, bins=total_year_bins, edgecolor='black')\n",
    "      plt.xlabel('Year')\n",
    "      plt.ylabel('Count')\n",
    "      plt.title(f'Diagnosis Years of Slide (whole dataset)')\n",
    "      plt.grid(False)\n",
    "      plt.show()\n",
    "      plt.clf()\n",
    "\n",
    "      label_1_year_list = []\n",
    "      label_0_year_list = []\n",
    "      for img_name in folders:\n",
    "        year = d_img[img_name]['year_of_initial_pathologic_diagnosis'] \n",
    "        OSS = d_img[img_name]['OSS']\n",
    "        DSS = d_img[img_name]['DSS']\n",
    "    \n",
    "        if OSS == 1 and DSS == 0:\n",
    "          label_0_year_list.append(year)\n",
    "        elif DSS == 1 and OSS == 0:\n",
    "          label_1_year_list.append(year)\n",
    "        elif DSS == 0 and OSS == 0:\n",
    "          print('are you using all data?')\n",
    "        elif DSS == 1 and OSS == 1:\n",
    "          print('false labeled')\n",
    "\n",
    "      # integer로 하려면 아래의 코드 참고 but 굳이 필요 없음\n",
    "      # fig, ax = plt.subplots(figsize=(6,4))\n",
    "      # ax.locator_params(axis='y', integer=True)\n",
    "      plt.hist(label_1_year_list, bins=total_year_bins, edgecolor='black')\n",
    "      plt.xlabel('Year')\n",
    "      plt.ylabel('Count')\n",
    "      plt.title(f'Diagnosis Years of Slide label_1 (whole dataset)')\n",
    "      plt.grid(False)\n",
    "      plt.show()\n",
    "      plt.clf()\n",
    "\n",
    "      plt.hist(label_0_year_list, bins=total_year_bins, edgecolor='black')\n",
    "      plt.xlabel('Year')\n",
    "      plt.ylabel('Count')\n",
    "      plt.title(f'Diagnosis Year of Slide label_0 (whole dataset)')\n",
    "      plt.grid(False)\n",
    "      plt.show()\n",
    "      plt.clf()\n",
    "    ##################################\n",
    "\n",
    "\n",
    "  if purpose == 'slide' or purpose == 'gradcam':\n",
    "    folders = [slide_name]\n",
    "  elif is_year_sort:\n",
    "    year_count = 0\n",
    "    current_year = 0\n",
    "    test_year = 2009\n",
    "    train_folders = []\n",
    "    val_folders = []\n",
    "    test_folders = []\n",
    "    for i, (folder, year) in enumerate(sorted_pairs):\n",
    "      if i == len(sorted_pairs) - 1:\n",
    "        if show_year_info:\n",
    "            pass\n",
    "            #print(f'year {current_year}, year_count {year_count}')\n",
    "      if current_year != year:\n",
    "        if current_year != 0:\n",
    "          if show_year_info:\n",
    "            pass\n",
    "            #print(f'year {current_year}, year_count {year_count}')\n",
    "        current_year = year\n",
    "        year_count = 0\n",
    "      year_count += 1\n",
    "      if year in [2002, 2011, 2012, 2013]:\n",
    "        test_folders.append(folder)\n",
    "      elif year in [2008, 2009]:\n",
    "        val_folders.append(folder)\n",
    "      else:\n",
    "        train_folders.append(folder)\n",
    "\n",
    "    if purpose == 'train':\n",
    "      folders = train_folders\n",
    "    elif purpose == 'val':\n",
    "      folders = val_folders\n",
    "    elif purpose == 'test':\n",
    "      folders = test_folders\n",
    "  else:\n",
    "    if purpose == 'train':\n",
    "      folders = folders[:int(len(folders)*ratio_train)]\n",
    "    elif purpose == 'val':\n",
    "      folders = folders[int(len(folders)*ratio_train):int(len(folders)*(ratio_train + ratio_val))]\n",
    "    elif purpose == 'test':\n",
    "      folders = folders[int(len(folders)*(ratio_train + ratio_val)):]\n",
    "    else:\n",
    "      print('wrong purpose')\n",
    "\n",
    "  if show_dataset_year_info and purpose != 'slide' and purpose != 'gradcam':\n",
    "    print('show_dataset_year_info!!')\n",
    "    year_list = []\n",
    "    for img_name in folders:\n",
    "      year = d_img[img_name]['year_of_initial_pathologic_diagnosis'] \n",
    "      year_list.append(year)\n",
    "    \n",
    "    # Pair each folder with its corresponding year\n",
    "    folder_year_pairs = zip(folders, year_list)\n",
    "\n",
    "    # Sort the pairs by year\n",
    "    sorted_pairs = sorted(folder_year_pairs, key=lambda x: x[1])\n",
    "\n",
    "    # Extract the sorted folders\n",
    "    # if we use monthly criteria, we will use sorted_folders\n",
    "    sorted_folders = [folder for folder, _ in sorted_pairs]\n",
    "\n",
    "\n",
    "    plt.hist(year_list, bins=total_year_bins, edgecolor='black')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Diagnosis Years of Slide ({purpose} dataset)')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    label_1_year_list = []\n",
    "    label_0_year_list = []\n",
    "    for img_name in folders:\n",
    "      year = d_img[img_name]['year_of_initial_pathologic_diagnosis'] \n",
    "      OSS = d_img[img_name]['OSS']\n",
    "      DSS = d_img[img_name]['DSS']\n",
    "  \n",
    "      if OSS == 1 and DSS == 0:\n",
    "        label_0_year_list.append(year)\n",
    "      elif DSS == 1 and OSS == 0:\n",
    "        label_1_year_list.append(year)\n",
    "      elif DSS == 0 and OSS == 0:\n",
    "        print('are you using all data?')\n",
    "      elif DSS == 1 and OSS == 1:\n",
    "        print('false labeled')\n",
    "\n",
    "    plt.hist(label_1_year_list, bins=total_year_bins, edgecolor='black')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Diagnosis Years of Slide label_1 ({purpose} dataset)')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.hist(label_0_year_list, bins=total_year_bins, edgecolor='black')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Diagnosis Year of Slide label_0 ({purpose} dataset)')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "  \n",
    "    \n",
    "  return folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_full_epochs : epochs_3_lr=0.001_08-22-09:39_full_epochs\n",
      "is_year_sort : True\n",
      "['TCGA-B0-5100', 'TCGA-B0-5102', 'TCGA-AK-3425', 'TCGA-BP-4158', 'TCGA-BP-4325', 'TCGA-B0-4810', 'TCGA-B0-4696', 'TCGA-B0-4814', 'TCGA-B0-4811', 'TCGA-B0-4815', 'TCGA-BP-4341', 'TCGA-BP-4169', 'TCGA-B0-4697', 'TCGA-CJ-5681', 'TCGA-BP-4335', 'TCGA-CJ-5679', 'TCGA-BP-4334', 'TCGA-CJ-6031', 'TCGA-B0-5699', 'TCGA-B0-4836', 'TCGA-B0-5709', 'TCGA-BP-4167', 'TCGA-BP-4756', 'TCGA-B0-4845', 'TCGA-B0-4833', 'TCGA-CJ-5678', 'TCGA-BP-4342', 'TCGA-CJ-6028', 'TCGA-B0-5693', 'TCGA-BP-4338', 'TCGA-B0-5812', 'TCGA-CJ-5680', 'TCGA-B0-4828', 'TCGA-BP-4166', 'TCGA-BP-4343', 'TCGA-BP-4959', 'TCGA-B0-4837', 'TCGA-B0-5706', 'TCGA-BP-4766', 'TCGA-BP-5180', 'TCGA-B0-5707', 'TCGA-BP-5174', 'TCGA-CJ-4874', 'TCGA-B0-4843', 'TCGA-CJ-6032', 'TCGA-CJ-4871', 'TCGA-CJ-4638', 'TCGA-CJ-4636', 'TCGA-BP-4960', 'TCGA-CJ-4640', 'TCGA-B0-4849', 'TCGA-B0-4852', 'TCGA-BP-4761', 'TCGA-BP-4760', 'TCGA-CJ-4634', 'TCGA-AK-3428', 'TCGA-CJ-4639', 'TCGA-BP-5170', 'TCGA-BP-4174', 'TCGA-CJ-4870', 'TCGA-B0-4842', 'TCGA-CJ-4923', 'TCGA-BP-5175', 'TCGA-BP-5177', 'TCGA-B0-4841', 'TCGA-CJ-4635', 'TCGA-AK-3429', 'TCGA-BP-5169', 'TCGA-B0-5711', 'TCGA-B0-4700', 'TCGA-CJ-4644', 'TCGA-AK-3433', 'TCGA-AK-3434', 'TCGA-CJ-4876', 'TCGA-B0-4848', 'TCGA-B0-5690', 'TCGA-BP-4759', 'TCGA-CJ-4875', 'TCGA-CJ-4878', 'TCGA-B0-4844', 'TCGA-CJ-4637', 'TCGA-AK-3427', 'TCGA-B0-5692', 'TCGA-BP-5178', 'TCGA-CJ-5677', 'TCGA-CJ-5682', 'TCGA-CJ-5671', 'TCGA-CJ-5683', 'TCGA-CJ-5675', 'TCGA-CJ-6033', 'TCGA-BP-4173', 'TCGA-BP-4765', 'TCGA-CJ-4869', 'TCGA-B0-5712', 'TCGA-BP-4758', 'TCGA-CJ-4881', 'TCGA-B0-4846', 'TCGA-CJ-5676', 'TCGA-CJ-4873', 'TCGA-AK-3426', 'TCGA-A3-3346', 'TCGA-A3-3347', 'TCGA-A3-3343', 'TCGA-A3-3307', 'TCGA-BP-4769', 'TCGA-A3-3320', 'TCGA-B0-4703', 'TCGA-A3-3331', 'TCGA-CJ-4885', 'TCGA-CZ-4856', 'TCGA-B0-5698', 'TCGA-CZ-5984', 'TCGA-A3-3374', 'TCGA-CZ-5982', 'TCGA-CJ-4918', 'TCGA-CJ-4897', 'TCGA-BP-4962', 'TCGA-BP-4964', 'TCGA-CJ-4890', 'TCGA-A3-3306', 'TCGA-CJ-4894', 'TCGA-CZ-4860', 'TCGA-CJ-4882', 'TCGA-B0-5080', 'TCGA-BP-4770', 'TCGA-AK-3440', 'TCGA-A3-3362', 'TCGA-CJ-5684', 'TCGA-B0-5083', 'TCGA-A3-3316', 'TCGA-CJ-4895', 'TCGA-BP-4963', 'TCGA-CJ-4886', 'TCGA-BP-4784', 'TCGA-B0-5081', 'TCGA-CJ-4892', 'TCGA-CZ-4861', 'TCGA-CJ-4893', 'TCGA-A3-3358', 'TCGA-CJ-4643', 'TCGA-CJ-4888', 'TCGA-BP-4781', 'TCGA-BP-4965', 'TCGA-CZ-4853', 'TCGA-BP-4967', 'TCGA-CZ-4859', 'TCGA-A3-3317', 'TCGA-BP-4771', 'TCGA-CJ-4889', 'TCGA-CZ-4858', 'TCGA-CJ-4884', 'TCGA-BP-4775', 'TCGA-CJ-5686', 'TCGA-BP-4776', 'TCGA-BP-4768', 'TCGA-CJ-4868', 'TCGA-CJ-4642', 'TCGA-BP-4777', 'TCGA-BP-4961', 'TCGA-A3-3359', 'TCGA-AK-3436', 'TCGA-BP-4176', 'TCGA-CJ-4641', 'TCGA-B0-4701', 'TCGA-BP-4782', 'TCGA-BP-4774', 'TCGA-CZ-4857', 'TCGA-CJ-4887', 'TCGA-A3-3357', 'TCGA-BP-4968', 'TCGA-A3-3365', 'TCGA-BP-4972', 'TCGA-A3-3378', 'TCGA-BP-4982', 'TCGA-CJ-4904', 'TCGA-CZ-4866', 'TCGA-BP-5181', 'TCGA-BP-4969', 'TCGA-CZ-4862', 'TCGA-AK-3453', 'TCGA-CZ-5986', 'TCGA-AK-3454', 'TCGA-B0-5713', 'TCGA-CZ-5457', 'TCGA-BP-4970', 'TCGA-A3-3326', 'TCGA-A3-3370', 'TCGA-CJ-4903', 'TCGA-BP-4344', 'TCGA-BP-4975', 'TCGA-CZ-5987', 'TCGA-BP-4987', 'TCGA-A3-3363', 'TCGA-AK-3451', 'TCGA-CZ-5461', 'TCGA-CZ-5985', 'TCGA-CJ-4902', 'TCGA-CZ-5458', 'TCGA-BP-4354', 'TCGA-CZ-4863', 'TCGA-AK-3447', 'TCGA-A3-3324', 'TCGA-A3-3349', 'TCGA-BP-4345', 'TCGA-BP-4985', 'TCGA-AK-3444', 'TCGA-CZ-5454', 'TCGA-CZ-5451', 'TCGA-A3-3335', 'TCGA-BP-4349', 'TCGA-AK-3445', 'TCGA-CZ-5455', 'TCGA-BP-4976', 'TCGA-CZ-5989', 'TCGA-CZ-5459', 'TCGA-AK-3450', 'TCGA-BP-4974', 'TCGA-BP-4983', 'TCGA-CJ-4907', 'TCGA-BP-4973', 'TCGA-A3-3383', 'TCGA-BP-4351', 'TCGA-CZ-5988', 'TCGA-CJ-4908', 'TCGA-A3-3322', 'TCGA-CJ-4901', 'TCGA-CJ-4900', 'TCGA-CZ-5456', 'TCGA-CJ-4912', 'TCGA-A3-3323', 'TCGA-BP-4971', 'TCGA-B0-5084', 'TCGA-BP-4986', 'TCGA-A3-3308', 'TCGA-A3-3319', 'TCGA-CZ-5452', 'TCGA-B0-4712', 'TCGA-CJ-4905', 'TCGA-A3-3387', 'TCGA-AK-3443', 'TCGA-CJ-4899', 'TCGA-BP-4977', 'TCGA-A3-3367', 'TCGA-BP-5199', 'TCGA-CZ-5460', 'TCGA-B0-5697', 'TCGA-BP-4999', 'TCGA-BP-4352', 'TCGA-BP-4798', 'TCGA-CZ-5467', 'TCGA-A3-3351', 'TCGA-BP-4989', 'TCGA-BP-4998', 'TCGA-BP-4991', 'TCGA-BP-5192', 'TCGA-A3-3372', 'TCGA-BP-4795', 'TCGA-BP-4992', 'TCGA-BP-5191', 'TCGA-CZ-5466', 'TCGA-AK-3460', 'TCGA-BP-4787', 'TCGA-BP-4995', 'TCGA-BP-5006', 'TCGA-BP-4799', 'TCGA-BP-5189', 'TCGA-BP-4993', 'TCGA-BP-4801', 'TCGA-BP-5001', 'TCGA-B0-5107', 'TCGA-BP-4994', 'TCGA-A3-3380', 'TCGA-BP-5185', 'TCGA-BP-5190', 'TCGA-AK-3458', 'TCGA-CZ-5464', 'TCGA-CZ-5463', 'TCGA-B0-5702', 'TCGA-A3-3373', 'TCGA-A3-3382', 'TCGA-AK-3461', 'TCGA-DV-5565', 'TCGA-BP-5184', 'TCGA-DV-5566', 'TCGA-DV-A4VZ', 'TCGA-BP-4789', 'TCGA-A3-3329', 'TCGA-B0-5696', 'TCGA-DV-5575', 'TCGA-BP-5000', 'TCGA-CJ-4916', 'TCGA-CZ-5468', 'TCGA-A3-3385', 'TCGA-CZ-5469', 'TCGA-BP-5004', 'TCGA-BP-4797', 'TCGA-A3-3328', 'TCGA-CZ-5462', 'TCGA-BP-5187', 'TCGA-BP-5008', 'TCGA-BP-5010', 'TCGA-BP-5007', 'TCGA-BP-5182', 'TCGA-BP-5009', 'TCGA-AK-3456', 'TCGA-B0-5695', 'TCGA-BP-5186', 'TCGA-BP-5183', 'TCGA-B8-5159', 'TCGA-EU-5905', 'TCGA-B2-3923', 'TCGA-B8-4153', 'TCGA-B0-5116', 'TCGA-B8-5551', 'TCGA-B2-5636', 'TCGA-B2-4102', 'TCGA-B4-5832', 'TCGA-B4-5844', 'TCGA-B8-5549', 'TCGA-B2-5639', 'TCGA-B8-5552', 'TCGA-B4-5835', 'TCGA-B2-5641', 'TCGA-B0-5120', 'TCGA-B0-4713', 'TCGA-AS-3778', 'TCGA-EU-5907', 'TCGA-B2-4099', 'TCGA-B8-4621', 'TCGA-B4-5378', 'TCGA-B0-5399', 'TCGA-B0-4710', 'TCGA-B0-5121', 'TCGA-B2-5635', 'TCGA-B8-5162', 'TCGA-EU-5906', 'TCGA-B8-5545', 'TCGA-B8-4620', 'TCGA-B4-5843', 'TCGA-B8-5550', 'TCGA-B4-5838', 'TCGA-B8-4619', 'TCGA-B8-5165', 'TCGA-B4-5836', 'TCGA-B0-5117', 'TCGA-B2-5633', 'TCGA-B2-4101', 'TCGA-B4-5834', 'TCGA-B8-5546', 'TCGA-B0-4718', 'TCGA-B0-4714', 'TCGA-B4-5377', 'TCGA-B8-4154', 'TCGA-B8-5164', 'TCGA-B0-5119', 'TCGA-B8-5158', 'TCGA-B0-5113', 'TCGA-B8-5163', 'TCGA-B8-5553', 'TCGA-B0-5402', 'TCGA-B8-4622']\n",
      "img cnt : 361\n",
      "(1073464, 32, 2)/(1073464,)\n",
      "OSS : 0, DSS : 0 = 0.0\n",
      "OSS : 0, DSS : 1 = 0.0\n",
      "OSS : 1, DSS : 0 = 0.0\n",
      "OSS : 1, DSS : 1 = 0.0\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "is_year_sort : True\n",
      "['TCGA-DV-A4VX', 'TCGA-BP-4804', 'TCGA-B0-5694', 'TCGA-G6-A8L7', 'TCGA-B0-5094', 'TCGA-BP-4807', 'TCGA-BP-5195', 'TCGA-BP-5198', 'TCGA-B0-5710', 'TCGA-BP-5201', 'TCGA-DV-5576', 'TCGA-AK-3465', 'TCGA-B0-4690', 'TCGA-BP-5194', 'TCGA-CZ-5470', 'TCGA-B0-5400', 'TCGA-BP-5200', 'TCGA-B0-4688', 'TCGA-BP-5196', 'TCGA-DV-5567', 'TCGA-DV-5573', 'TCGA-B0-4707', 'TCGA-BP-4803', 'TCGA-DV-5574', 'TCGA-BP-5202', 'TCGA-B0-5092', 'TCGA-B8-4151', 'TCGA-B0-5115', 'TCGA-B0-5703', 'TCGA-B0-5110', 'TCGA-EU-5904', 'TCGA-B8-4146', 'TCGA-B0-5700', 'TCGA-DV-5568', 'TCGA-B0-4691', 'TCGA-B0-5108', 'TCGA-AS-3777', 'TCGA-B0-5109', 'TCGA-B8-4148', 'TCGA-DV-5569']\n",
      "img cnt : 40\n",
      "(131741, 32, 2)/(131741,)\n",
      "OSS : 0, DSS : 0 = 0.0\n",
      "OSS : 0, DSS : 1 = 0.0\n",
      "OSS : 1, DSS : 0 = 0.0\n",
      "OSS : 1, DSS : 1 = 0.0\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "is_year_sort : True\n",
      "['TCGA-BP-4160', 'TCGA-B8-4143', 'TCGA-BP-4330', 'TCGA-BP-4165', 'TCGA-BP-4329', 'TCGA-BP-4159', 'TCGA-BP-4326', 'TCGA-BP-4332', 'TCGA-BP-4163', 'TCGA-B0-4827', 'TCGA-B0-5691', 'TCGA-B0-4821', 'TCGA-BP-4162', 'TCGA-B0-4822', 'TCGA-B0-5705', 'TCGA-B0-4818', 'TCGA-BP-4161', 'TCGA-B0-4816', 'TCGA-A3-A8OU', 'TCGA-A3-A6NI', 'TCGA-B8-A54E', 'TCGA-A3-A8OV', 'TCGA-A3-A8CQ', 'TCGA-A3-A8OW', 'TCGA-B8-A54D', 'TCGA-B8-A54I', 'TCGA-A3-A6NN', 'TCGA-B8-A54H', 'TCGA-GK-A6C7', 'TCGA-MM-A563', 'TCGA-B8-A54K', 'TCGA-B8-A54F', 'TCGA-A3-A6NL', 'TCGA-B2-A4SR', 'TCGA-B8-A54J', 'TCGA-A3-A8OX', 'TCGA-MM-A564', 'TCGA-B8-A54G', 'TCGA-A3-A6NJ', 'TCGA-G6-A5PC', 'TCGA-G6-A8L6', 'TCGA-6D-AA2E', 'TCGA-B8-A8YJ', 'TCGA-T7-A92I', 'TCGA-B8-A7U6']\n",
      "img cnt : 45\n",
      "(127603, 32, 2)/(127603,)\n",
      "OSS : 0, DSS : 0 = 0.0\n",
      "OSS : 0, DSS : 1 = 0.0\n",
      "OSS : 1, DSS : 0 = 0.0\n",
      "OSS : 1, DSS : 1 = 0.0\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "PATH_val : epochs_3_lr=0.001_08-22-09:39_use_val\n",
      "lr : 0.001\n",
      "총 배치의 수 : 1073464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e21c3d3a14beab3dc4ad59e89eac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1073464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f31aafd3abca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m   \u001b[0mMTR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_val_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f31aafd3abca>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, use_val)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.init\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "# 1. initialize process group\n",
    "if is_multi_gpu:\n",
    "  os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "  os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2,3\"  # Set the GPUs 2 and 3 to use\n",
    "  import torch.distributed as dist\n",
    "  from torch.nn.parallel import DistributedDataParallel\n",
    "  dist.init_process_group(\"nccl\")\n",
    "  rank = dist.get_rank()\n",
    "  torch.cuda.set_device(rank)\n",
    "  device = torch.cuda.current_device()\n",
    "  world_size = dist.get_world_size()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, root, d_img, purpose='train', is_pixel_value_0_to_1=True, slide_name=None, show_detail=True, is_year_sort=False, show_year_info=False, show_dataset_year_info=False, patches_num=patches_num):\n",
    "\n",
    "      self.root = root\n",
    "      self.regions_root = f'{root}/cropped_regions'\n",
    "      self.coords_root = f'{root}/patch_coords'\n",
    "      self.origins_root = f'{root}/region_globals_origins' # 얘는 어차피 일단 학습에는 안쓰임! => 나중에 얘까지 쓰는 버전으로 작게 테스트 해본 뒤에 전체 학습 시작해야함\n",
    "\n",
    "      self.whole_coords_data_path = f'{root}/preprocessed_data/whole_coords_{purpose}{is_year_sort}{ratio_train}.npy'\n",
    "      self.whole_labels_data_path = f'{root}/preprocessed_data/whole_labels_{purpose}{is_year_sort}{ratio_train}.npy'\n",
    "      self.whole_region_names_data_path = f'{root}/preprocessed_data/whole_region_names_{purpose}{is_year_sort}{ratio_train}.npy'\n",
    "\n",
    "      self.file_list = [] # patch file\n",
    "      self.target_slide_list = []\n",
    "      self.labels = None\n",
    "      self.d_img = d_img\n",
    "      self.purpose = purpose\n",
    "      self.is_pixel_value_0_to_1 = is_pixel_value_0_to_1\n",
    "      self.label_table = np.zeros((2,2))\n",
    "      self.show_detail = show_detail\n",
    "      self.patches_num = patches_num\n",
    "      self.svs_openslide_list = []\n",
    "      \n",
    "      self.whole_coords_data = None\n",
    "\n",
    "      self.region_file_name_data = None\n",
    "\n",
    "      #여기 patch_samples_root 안쓰게 통일해줘야함\n",
    "      self.target_slide_list = make_target_slide_list(patch_samples_root, d_img, purpose=purpose, slide_name=slide_name, is_year_sort=is_year_sort ,show_year_info=show_year_info, show_dataset_year_info=show_dataset_year_info)\n",
    "      if self.show_detail:\n",
    "        print(self.target_slide_list)\n",
    "        print(f'img cnt : {len(self.target_slide_list)}')\n",
    "\n",
    "\n",
    "      if os.path.exists(self.whole_coords_data_path):\n",
    "        self.whole_coords_data = np.load(self.whole_coords_data_path)\n",
    "        self.region_file_name_data = np.load(self.whole_region_names_data_path)\n",
    "        self.labels = np.load(self.whole_labels_data_path)\n",
    "      else:\n",
    "        cnt_regions = self.get_valid_regions_count()\n",
    "        print(f'cnt_regions:{cnt_regions}')\n",
    "        whole_coords_data = np.zeros((cnt_regions,32,2), dtype=np.uint8)\n",
    "        self.region_file_name_data = np.zeros((cnt_regions), dtype='a20') # 20개의 character를 저장가능\n",
    "        self.labels = np.zeros((cnt_regions))\n",
    "        idx_regions = 0\n",
    "\n",
    "        for img_name in self.target_slide_list:\n",
    "          OSS = d_img[img_name]['OSS']\n",
    "          DSS = d_img[img_name]['DSS']\n",
    "          svs_root = d_img[img_name]['svs_root']\n",
    "          slide = openslide.OpenSlide(svs_root)\n",
    "          self.svs_openslide_list.append(slide)\n",
    "          \n",
    "          if OSS == 1 and DSS == 0:\n",
    "            label = np.zeros((1))\n",
    "          elif DSS == 1 and OSS == 0:\n",
    "            label = np.ones((1))\n",
    "          elif DSS == 0 and OSS == 0:\n",
    "            print('are you using all data?')\n",
    "          elif DSS == 1 and OSS == 1:\n",
    "            print('false labeled')\n",
    "            \n",
    "          self.label_table[OSS][DSS] += 1\n",
    "          # self.labels.append(label)\n",
    "\n",
    "          coords_path = f'{self.coords_root}/{img_name}'\n",
    "          # print(os.listdir(img_root)[:3]) 패치들이 순서대로 list에 삽입되지는 않는다.\n",
    "          # print(len(os.listdir(img_root)))\n",
    "          \n",
    "          coords_file_list = os.listdir(coords_path)\n",
    "          for coords_file_name in coords_file_list:\n",
    "            coords_file_path = f'{coords_path}/{coords_file_name}'\n",
    "            coords = np.load(coords_file_path)\n",
    "            \n",
    "            \n",
    "            if coords.shape[0] == 32:\n",
    "              whole_coords_data[idx_regions] = coords.astype(int)\n",
    "              region_file_name = f'{coords_file_name[:-11]}' # 11개를 잘라먹겠다는 뜻임\n",
    "              self.region_file_name_data[idx_regions] = region_file_name\n",
    "              self.labels[idx_regions] = label\n",
    "              idx_regions += 1\n",
    "\n",
    "              # print(f'{self.regions_root}/{img_name}/{coords_file_name[:-11]}.npy')\n",
    "      \n",
    "        self.whole_coords_data = whole_coords_data\n",
    "        np.save(self.whole_coords_data_path, self.whole_coords_data)\n",
    "        np.save(self.whole_region_names_data_path, self.region_file_name_data)\n",
    "        np.save(self.whole_labels_data_path, np.array(self.labels))\n",
    "\n",
    "\n",
    "      print(f'{self.whole_coords_data.shape}/{self.region_file_name_data.shape}')\n",
    "      self.labels = torch.FloatTensor(self.labels).unsqueeze(1)  \n",
    "\n",
    "      for OSS in [0,1]:\n",
    "        for DSS in [0,1]:\n",
    "          if self.show_detail:\n",
    "            print(f\"OSS : {OSS}, DSS : {DSS} = {self.label_table[OSS][DSS]}\")\n",
    "      if self.show_detail:\n",
    "        print(self.label_table)\n",
    "\n",
    "  def get_target_slide_list(self):\n",
    "    return self.target_slide_list\n",
    "\n",
    "  def get_valid_regions_count(self):\n",
    "    if self.purpose == 'train':\n",
    "      return 1073464 # 이거 train, test, val 다르기 때문에 그냥 직접계산하는게 맞다\n",
    "    elif self.purpose == 'val':\n",
    "      return 131741\n",
    "    elif self.purpose == 'test':\n",
    "      return 127603\n",
    "\n",
    "    cnt_regions = 0\n",
    "    for img_name in self.target_slide_list:\n",
    "      coords_path = f'{self.coords_root}/{img_name}'    \n",
    "      coords_file_list = os.listdir(coords_path)\n",
    "      for coords_file_name in coords_file_list:\n",
    "          coords_file_path = f'{coords_path}/{coords_file_name}'\n",
    "          coords = np.load(coords_file_path)\n",
    "          if cnt_regions == 0:\n",
    "            print(coords.dtype)\n",
    "          if coords.shape[0] == 32:\n",
    "            cnt_regions += 1\n",
    "\n",
    "    print(cnt_regions)\n",
    "    return cnt_regions\n",
    "  \n",
    "  def numpy_to_tensor(self, x_data):\n",
    "    x_data = torch.FloatTensor(x_data)\n",
    "    x_data = x_data.permute(2, 0, 1)  # Permute the dimensions\n",
    "    return x_data\n",
    "\n",
    "  def __len__(self):\n",
    "    # coord_list는 svs당 하나\n",
    "    return (len(self.region_file_name_data) * 32) // 4 # Since we are getting two items at a time\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    idx = 4*idx\n",
    "    idx_2 = idx + 1\n",
    "    idx_3 = idx + 2\n",
    "    idx_4 = idx + 3\n",
    "    # numpy로 사진 저장하기 (/255는 numpy uint 형식으로 저장 )\n",
    "    # 한장 슬라이드에서 나중에 \n",
    "    # 32 : coords per region\n",
    "    # 이렇게 똑같이 하는데, region에서 뽑은거랑 svs에서 뽑는거랑 비교해보기\n",
    "    region_num = idx // 32\n",
    "    region_file_name = self.region_file_name_data[region_num]\n",
    "    region_file_name = region_file_name.decode('utf-8')\n",
    "    region = np.load(f'{self.regions_root}/{region_file_name[:12]}/{region_file_name}.npy')\n",
    "\n",
    "\n",
    "    coord = self.whole_coords_data[region_num][idx % 32]\n",
    "    x = coord[0]\n",
    "    y = coord[1]\n",
    "    patch = region[y:y+224, x:x+224]\n",
    "    x_data = self.numpy_to_tensor(patch)\n",
    "    x_data = x_data / 255\n",
    "    label = self.labels[region_num]\n",
    "\n",
    "    coord = self.whole_coords_data[region_num][idx_2 % 32]\n",
    "    x = coord[0]\n",
    "    y = coord[1]\n",
    "    patch = region[y:y+224, x:x+224]\n",
    "    x_data_2 = self.numpy_to_tensor(patch)\n",
    "    x_data_2 = x_data / 255\n",
    "    label_2 = self.labels[region_num]\n",
    "\n",
    "    coord = self.whole_coords_data[region_num][idx_3 % 32]\n",
    "    x = coord[0]\n",
    "    y = coord[1]\n",
    "    patch = region[y:y+224, x:x+224]\n",
    "    x_data_3 = self.numpy_to_tensor(patch)\n",
    "    x_data_3 = x_data / 255\n",
    "    label_3 = self.labels[region_num]\n",
    "\n",
    "    coord = self.whole_coords_data[region_num][idx_4 % 32]\n",
    "    x = coord[0]\n",
    "    y = coord[1]\n",
    "    patch = region[y:y+224, x:x+224]\n",
    "    x_data_4 = self.numpy_to_tensor(patch)\n",
    "    x_data_4 = x_data / 255\n",
    "    label_4 = self.labels[region_num]\n",
    "\n",
    "    return (x_data, label), (x_data_2, label_2), (x_data_3, label_3), (x_data_4, label_4)\n",
    "\n",
    "\n",
    "class ModelTrain:\n",
    "  def __init__(self, PATH, PATH_full_epochs, model, dataset, val_dataset, test_dataset, criterion, optimizer, training_epochs=30, batch_size=32, shuffle=True, learning_rate=0.001):\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.criterion = criterion\n",
    "    self.optimizer = optimizer\n",
    "    self.training_epochs = training_epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.num_workers = 2\n",
    "\n",
    "    if is_multi_gpu:\n",
    "      train_sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        shuffle=True,\n",
    "      )\n",
    "      self.data_loader = DataLoader(dataset=dataset, sampler=train_sampler, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=self.num_workers,pin_memory=True) # 미니배치사이즈를 말한다\n",
    "    else:\n",
    "      self.data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=self.num_workers,pin_memory=True) # 미니배치사이즈를 말한다\n",
    "    self.val_data_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=self.num_workers, pin_memory=True) # 미니배치사이즈를 말한다\n",
    "    self.test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=self.num_workers, pin_memory=True) # 미니배치사이즈를 말한다\n",
    "    self.PATH = PATH + '_use_val.pt'\n",
    "    self.PATH_full_epochs = PATH + '_full_epochs.pt'\n",
    "    self.PATH_first = PATH + '_first.pt'\n",
    "    self.scheduler = StepLR(self.optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    print(f'lr : {self.optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "\n",
    "  def train_model(self, use_val=False):\n",
    "    costs = []\n",
    "    val_loss_list = []\n",
    "    test_loss_list = []\n",
    "    min_val_loss = 30000\n",
    "    min_val_loss_epoch = 1\n",
    "    best_model = None\n",
    "    best_model_optimizer = None\n",
    "\n",
    "    total_batch = len(self.data_loader)\n",
    "    val_total_batch = len(self.val_data_loader)\n",
    "    test_total_batch = len(self.test_data_loader)\n",
    "    print('총 배치의 수 : {}'.format(total_batch))\n",
    "    for epoch in range(self.training_epochs):\n",
    "      self.model.train()\n",
    "      # if epoch == 1 or epoch == 3:\n",
    "      #   self.scheduler.step()\n",
    "      avg_cost = 0\n",
    "      if show_pbar:\n",
    "        pbar = tqdm(total = total_batch)\n",
    "      for (X1, Y1), (X2, Y2), (X3, Y3), (X4, Y4) in self.data_loader:\n",
    "        \n",
    "        # Combine the two batches\n",
    "        X = torch.cat([X1, X2, X3, X4], dim=0).to(device)\n",
    "        Y = torch.cat([Y1, Y2, Y3, Y4], dim=0).to(device)\n",
    "\n",
    "        hypothesis = self.model(X)\n",
    "        cost = self.criterion(hypothesis, Y)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "        if show_pbar:\n",
    "          pbar.update(1)\n",
    "\n",
    "      costs.append(avg_cost.cpu().detach().numpy())\n",
    "      \n",
    "      if use_val == True:\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "          for (X1, Y1), (X2, Y2), (X3, Y3), (X4, Y4) in self.val_data_loader:\n",
    "            # Combine the two batches\n",
    "            X = torch.cat([X1, X2, X3, X4], dim=0).to(device)\n",
    "            Y = torch.cat([Y1, Y2, Y3, Y4], dim=0).to(device)\n",
    "            hypothesis = self.model(X)\n",
    "            loss = self.criterion(hypothesis, Y)\n",
    "            val_loss += loss / val_total_batch\n",
    "          \n",
    "          for (X1, Y1), (X2, Y2), (X3, Y3), (X4, Y4) in self.test_data_loader:\n",
    "            # Combine the two batches\n",
    "            X = torch.cat([X1, X2, X3, X4], dim=0).to(device)\n",
    "            Y = torch.cat([Y1, Y2, Y3, Y4], dim=0).to(device)\n",
    "            hypothesis = self.model(X)\n",
    "            loss = self.criterion(hypothesis, Y)\n",
    "            test_loss += loss / test_total_batch\n",
    "\n",
    "        val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "        test_loss_list.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "          min_val_loss = val_loss\n",
    "          min_val_loss_epoch = epoch+1\n",
    "          best_model = self.model.state_dict()\n",
    "          best_model_optimizer = self.optimizer.state_dict()\n",
    "          # 추후에 deepcopy로 바꾸기. 위의 것 자체가 object pointer를 반환하는 듯 하다...\n",
    "          torch.save({\n",
    "            'epoch': min_val_loss_epoch,\n",
    "            'model_state_dict': best_model,\n",
    "            'optimizer_state_dict': best_model_optimizer,\n",
    "            'loss': min_val_loss,\n",
    "          }, self.PATH)\n",
    "\n",
    "        \n",
    "          # if self.optimizer.param_groups[0][\"lr\"] < 1e-5: self.optimizer.param_groups[0][\"lr\"] = 1e-5\n",
    "\n",
    "\n",
    "        # elif epoch > 0:\n",
    "        #   self.optimizer.param_groups[0][\"lr\"] *= 0.1\n",
    "        #   print(f'epoch+1 : {epoch+1}, learning_rate : {self.optimizer.param_groups[0][\"lr\"]}')\n",
    "        #   if self.optimizer.param_groups[0][\"lr\"] < 1e-5: self.optimizer.param_groups[0][\"lr\"] = 1e-5\n",
    "\n",
    "\n",
    "\n",
    "      print('[Epoch: {:>4}] cost = {:>.5}  val_loss = {:>.5} test_loss = {:>.5}'.format(epoch + 1, avg_cost, val_loss, test_loss))\n",
    "      print(f'Epoch : {epoch+1}, learning_rate : {self.optimizer.param_groups[0][\"lr\"]}')\n",
    "      if epoch+1 == 2: # epoch 3부터 10배 작아진 learning rate 적용\n",
    "        self.optimizer.param_groups[0][\"lr\"] *= 0.1\n",
    "      \n",
    "\n",
    "    print(f'lowest loss on validation set : {min_val_loss}')\n",
    "\n",
    "\n",
    "    torch.save({\n",
    "      'epoch': self.training_epochs,\n",
    "      'model_state_dict': self.model.state_dict(),\n",
    "      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "      'loss': costs[-1],\n",
    "    }, self.PATH_full_epochs)\n",
    "\n",
    "    x = range(1, len(costs)+1)\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.plot(x, costs, 'b')\n",
    "    plt.plot(x, val_loss_list, 'r')\n",
    "    plt.plot(x, test_loss_list, 'g')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(x)\n",
    "    plt.savefig('savefig_loss_graph.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "class LoadPretrainedModel:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def load_model_ResNet_modified(self, version='resnet18'):\n",
    "    # Load pre-trained ResNet18 model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # Modify the first layer of the model to accept 3x64x64 input\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = LoadPretrainedModel().load_model_ResNet_modified().to(device)\n",
    "\n",
    "if is_continous_training:\n",
    "  checkpoint = torch.load('epochs_10_lr=0.001_04-12-18:20_use_val.pt')\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "intend_over_fitting = False\n",
    "\n",
    "if intend_over_fitting:\n",
    "    learning_rate = 0.0003 # linearization 가정을 만족할 만큼 작은 스텝사이즈여야!\n",
    "    training_epochs = 15 # 이 이상부터 오버피팅 일어남 (7로 했을때)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "timestamp = time.strftime('%m-%d-%H:%M', time.localtime(time.time()))\n",
    "\n",
    "PATH = f\"epochs_{training_epochs}_lr={learning_rate}_{timestamp}\"\n",
    "PATH_full_epochs = f\"epochs_{training_epochs}_lr={learning_rate}_{timestamp}\"\n",
    "\n",
    "print(f'PATH_full_epochs : {PATH_full_epochs}_full_epochs')\n",
    "\n",
    "if is_cross_model_test:\n",
    "  pass\n",
    "  # 그냥 직접 따져주는게 더 정확 (model이랑 sort랑 반대로 해주면 cross model test다)\n",
    "  # is_cross_model_year_sort = not is_year_sort # is_year_sort\n",
    "  cross_dataset = CustomDataset(root = slide_info_root, d_img=d_img, purpose='train', is_year_sort=is_year_sort, show_year_info=show_year_info, show_dataset_year_info=show_dataset_year_info)\n",
    "  cross_val_dataset = CustomDataset(root = slide_info_root, d_img=d_img, purpose='val', is_year_sort=is_year_sort, show_year_info=False, show_dataset_year_info=show_dataset_year_info)\n",
    "  cross_test_dataset = CustomDataset(root = slide_info_root, d_img=d_img, purpose='test', is_year_sort=is_year_sort, show_year_info=False, show_dataset_year_info=show_dataset_year_info)\n",
    "  cross_slide_list = cross_dataset.get_target_slide_list()\n",
    "  cross_val_slide_list = cross_val_dataset.get_target_slide_list()\n",
    "  cross_test_slide_list = cross_test_dataset.get_target_slide_list()\n",
    "\n",
    "  print(cross_slide_list)\n",
    "  print(cross_val_slide_list)\n",
    "  print(cross_test_slide_list)\n",
    "dataset = CustomDataset(root = slide_info_root, d_img=d_img, purpose='train', is_year_sort=is_year_sort, show_year_info=show_year_info, show_dataset_year_info=show_dataset_year_info)\n",
    "val_dataset = CustomDataset(root = slide_info_root, d_img=d_img, purpose='val', is_year_sort=is_year_sort, show_year_info=False, show_dataset_year_info=show_dataset_year_info)\n",
    "test_dataset = CustomDataset(root = slide_info_root, d_img=d_img, purpose='test', is_year_sort=is_year_sort, show_year_info=False, show_dataset_year_info=show_dataset_year_info)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "print(f'PATH_val : {PATH}_use_val')\n",
    "if is_multi_gpu:\n",
    "  model = DistributedDataParallel(model, device_ids=[device], output_device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if use_weight_decay:\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "MTR = ModelTrain(PATH=PATH, PATH_full_epochs=PATH_full_epochs, model=model, dataset=dataset, val_dataset=val_dataset, test_dataset=test_dataset, criterion=criterion, optimizer=optimizer, training_epochs=training_epochs, batch_size=batch_size, shuffle=True, learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "if train:\n",
    "\n",
    "  MTR.train_model(use_val=True)\n",
    "\n",
    "if train and use_val_model:\n",
    "    model = LoadPretrainedModel().load_model_ResNet_modified().to(device)\n",
    "    checkpoint = torch.load(f'{PATH}_use_val.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "if use_exist_model:\n",
    "    if is_year_sort_model and is_l2_norm_model:\n",
    "      PATH = 'epochs_7_lr=0.001_05-13-14:46_use_val' # l2norm 0.001 year sort\n",
    "      PATH = 'epochs_3_lr=0.001_06-25-16:45_use_val'\n",
    "    elif is_year_sort_model and not is_l2_norm_model:\n",
    "      #PATH = 'epochs_8_lr=0.001_05-08-21:20_use_val' # year sort\n",
    "      PATH = 'epochs_3_lr=0.001_06-18-15:11_use_val'\n",
    "    elif not is_year_sort_model and is_l2_norm_model:\n",
    "      PATH = 'epochs_7_lr=0.001_05-11-19:49_use_val' # l2norm 0.001 original sort\n",
    "    elif not is_year_sort_model and not is_l2_norm_model:\n",
    "      PATH = 'epochs_10_lr=0.001_04-12-18:20_use_val' # original sort\n",
    "      # [Epoch: 2] cost = 0.1942  val_loss = 0.5083 test_loss = 0.7353\n",
    "    #PATH = 'epochs_8_lr=0.001_05-06-22:34_use_val' \n",
    "\n",
    "    model = LoadPretrainedModel().load_model_ResNet_modified().to(device)\n",
    "    checkpoint = torch.load(f'{PATH}.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f'current using model : {PATH}')\n",
    "\n",
    "# 10분걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTest:\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "  \n",
    "  def test_on_target_set(self, dataset, test_name='accuracy', return_accuracy=False, show_pbar=show_pbar, show_visualizer=False):\n",
    "    # Set the model to evaluation mode\n",
    "    model = self.model\n",
    "    model.eval()\n",
    "\n",
    "    num_workers = 2\n",
    "    if show_visualizer:\n",
    "      num_workers = 1\n",
    "    # 이거 map이용해서 multi로 돌리기!\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    print(len(dataset))\n",
    "    data_len = len(dataset)\n",
    "    batch_len = len(data_loader)\n",
    "    whole_y_pred_prob = np.zeros((data_len,1))\n",
    "    whole_y_pred_binary = np.zeros((data_len,1))\n",
    "    whole_y_target = np.zeros((data_len,1))\n",
    "    print(whole_y_pred_prob.shape)\n",
    "\n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    if show_pbar:\n",
    "      pbar = tqdm(total = batch_len)\n",
    "    for X, Y in data_loader:\n",
    "      X = X.to(device)\n",
    "      Y = Y.to(device)\n",
    "\n",
    "      hypothesis = model(X)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          y_pred_prob = model(X)\n",
    "\n",
    "      # Convert the predicted probabilities to binary predictions\n",
    "      y_pred_binary = torch.round(y_pred_prob)\n",
    "      \n",
    "      if Y.shape[0] == 32:\n",
    "        whole_y_pred_prob[i*32:(i+1)*32] = y_pred_prob.cpu().detach().numpy()\n",
    "        whole_y_pred_binary[i*32:(i+1)*32] = y_pred_binary.cpu().detach().numpy()\n",
    "        whole_y_target[i*32:(i+1)*32] = Y.cpu().detach().numpy()\n",
    "      else: \n",
    "        whole_y_pred_prob[i*32:i*32 + Y.shape[0]] = y_pred_prob.cpu().detach().numpy()\n",
    "        whole_y_pred_binary[i*32:i*32 + Y.shape[0]] = y_pred_binary.cpu().detach().numpy()\n",
    "        whole_y_target[i*32:i*32 + Y.shape[0]] = Y.cpu().detach().numpy()\n",
    "\n",
    "      i += 1\n",
    "      # Calculate the accuracy\n",
    "      correct = (y_pred_binary == Y).sum().item()\n",
    "      total_correct += correct\n",
    "\n",
    "      if show_pbar:\n",
    "        pbar.update(1)\n",
    "    \n",
    "    accuracy = total_correct / data_len\n",
    "\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "    if return_accuracy:\n",
    "      return whole_y_target, whole_y_pred_binary, whole_y_pred_prob, accuracy\n",
    "\n",
    "    return whole_y_target, whole_y_pred_binary, whole_y_pred_prob\n",
    "     \n",
    "model.eval()\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc as auc_ftn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MTE = ModelTest(model=model)\n",
    "\n",
    "if show_accuracy:\n",
    "    with torch.no_grad():\n",
    "        y_val, y_val_pred_binary, y_val_pred_prob = MTE.test_on_target_set(val_dataset)\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_val, y_val_pred_prob)\n",
    "    auc = metrics.roc_auc_score(y_val, y_val_pred_prob)\n",
    "\n",
    "    #create ROC curve\n",
    "    plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('savefig_val_auc.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # PR_curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val,  y_val_pred_prob)\n",
    "    area = auc_ftn(recall, precision)\n",
    "\n",
    "    plt.plot(recall, precision, label=f\"PR curve (AUC = {area:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('savefig_val_pr.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_test, y_test_pred_binary, y_test_pred_prob = MTE.test_on_target_set(test_dataset)\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_test_pred_prob)\n",
    "    auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "\n",
    "    #create ROC curve\n",
    "    plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('savefig_test_auc.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # PR_curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test,  y_test_pred_prob)\n",
    "    area = auc_ftn(recall, precision)\n",
    "\n",
    "    plt.plot(recall, precision, label=f\"PR curve (AUC = {area:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('savefig_test_pr.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_train, y_train_pred_binary, y_train_pred_prob = MTE.test_on_target_set(dataset)\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_train[:],  y_train_pred_prob[:])\n",
    "    auc = metrics.roc_auc_score(y_train[:], y_train_pred_prob[:])\n",
    "\n",
    "    #create ROC curve\n",
    "    plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('savefig_train_auc.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # PR_curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train[:],  y_train_pred_prob[:])\n",
    "    area = auc_ftn(recall, precision)\n",
    "\n",
    "    plt.plot(recall, precision, label=f\"PR curve (AUC = {area:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('savefig_train_pr.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def slide_accuracy_test_on_target_set(target_slide_list, d_img, use_threshold=False, threshold=None, show_visualizer=False, show_label_accuracy=True, show_label_only=None, is_cross_model_test=False, dataset_name=None):\n",
    "  \n",
    "  cnt_correct = 0\n",
    "  cnt_wrong = 0\n",
    "  label_1_cnt = 0\n",
    "  label_1_correct_cnt = 0\n",
    "  label_0_cnt = 0\n",
    "  label_0_correct_cnt = 0\n",
    "\n",
    "  if use_threshold:\n",
    "    t_label_1_correct_cnt = 0\n",
    "    t_label_0_correct_cnt = 0\n",
    "    t_cnt_correct = 0\n",
    "    t_cnt_wrong = 0\n",
    "\n",
    "  model_info_slide_dataset = None\n",
    "\n",
    "  if not is_cross_model_test:\n",
    "    if dataset_name == 'val':\n",
    "      model_info_slide_dataset = 'unseen(val)'\n",
    "    elif dataset_name == 'test':\n",
    "      model_info_slide_dataset = 'unseen(test)'\n",
    "    elif dataset_name == 'train':\n",
    "      model_info_slide_dataset = 'seen(train)'\n",
    "      \n",
    "  for slide_name in target_slide_list:\n",
    "    if is_cross_model_test:\n",
    "      is_unseen_slide = False\n",
    "      for cross_val_slide in cross_val_slide_list:\n",
    "        if slide_name == cross_val_slide:\n",
    "          print('this slide is in cross val slide list')\n",
    "          is_unseen_slide = True\n",
    "          model_info_slide_dataset = 'unseen(val)'\n",
    "          break\n",
    "      for cross_test_slide in cross_test_slide_list:\n",
    "        if slide_name == cross_test_slide:\n",
    "          print('this slide is in cross test slide list')\n",
    "          is_unseen_slide = True\n",
    "          model_info_slide_dataset = 'unseen(test)'\n",
    "          break\n",
    "      if not is_unseen_slide:\n",
    "        print('this slide is in cross train slide list')\n",
    "        model_info_slide_dataset = 'seen(train)'\n",
    "\n",
    "\n",
    "\n",
    "    slide_dataset = CustomDataset(root = patch_samples_root, d_img=d_img, purpose='slide', slide_name=slide_name, show_detail=False)\n",
    "    \n",
    "    y_target, y_target_pred_binary_each, y_target_pred_prob_each, img_accuracy = MTE.test_on_target_set(slide_dataset, return_accuracy=True, show_pbar=False, show_visualizer=show_visualizer)\n",
    "    label = y_target[0]\n",
    "    \n",
    "    print(f'slide_name : {slide_name} / label : {label}')\n",
    "\n",
    "    cnt_1 = 0\n",
    "    cnt_0 = 0\n",
    "\n",
    "    for pred in y_target_pred_binary_each:\n",
    "        if pred == 1:\n",
    "            cnt_1 += 1\n",
    "        else:\n",
    "            cnt_0 += 1\n",
    "    \n",
    "    if cnt_1 > cnt_0:\n",
    "        voted_pred = 1\n",
    "    else:\n",
    "        voted_pred = 0\n",
    "\n",
    "    if use_threshold:\n",
    "      if cnt_1 / (cnt_1 + cnt_0) > threshold:\n",
    "        t_voted_pred = 1\n",
    "      else:\n",
    "        t_voted_pred = 0\n",
    "    \n",
    "    if label == 1:\n",
    "      label_1_cnt += 1\n",
    "    else:\n",
    "      label_0_cnt += 1\n",
    "\n",
    "    if label == voted_pred:\n",
    "        print('correct')\n",
    "        cnt_correct += 1\n",
    "        if label == 1:\n",
    "          label_1_correct_cnt += 1\n",
    "        else:\n",
    "          label_0_correct_cnt += 1\n",
    "    else:\n",
    "        print('wrong')\n",
    "        cnt_wrong += 1\n",
    "\n",
    "    if use_threshold:\n",
    "      if label == t_voted_pred:\n",
    "        print(f'correct (threshold for label_1 : {threshold*100}%)')\n",
    "        t_cnt_correct += 1\n",
    "        if label == 1:\n",
    "          t_label_1_correct_cnt += 1\n",
    "        else:\n",
    "          t_label_0_correct_cnt += 1\n",
    "      else:\n",
    "          print(f'wrong (threshold for label_1 : {threshold*100}%)')\n",
    "          t_cnt_wrong += 1\n",
    "\n",
    "    if show_visualizer:\n",
    "      if show_label_only != None:\n",
    "        if label != show_label_only:\n",
    "          continue\n",
    "\n",
    "      img_info = d_img[slide_name]\n",
    "      img_root = img_info['img_root']\n",
    "      img = cv2.imread(img_root)\n",
    "      ###########\n",
    "      fig, ax = plt.subplots(figsize=(10, 10))\n",
    "      plt.title(f'{slide_name} / label={int(label[0])}')\n",
    "      plt.imshow(img[..., ::-1])\n",
    "      ###########\n",
    "\n",
    "      #continue\n",
    "\n",
    "      fig, ax = plt.subplots(figsize=(10, 10))\n",
    "      plt.title(slide_name + f': {model_info_slide_dataset}' + ' / Accuracy: {:.2f}% / '.format(img_accuracy * 100) + \"model: \" + model_name)\n",
    "      ax.imshow(img[..., ::-1])\n",
    "\n",
    "      target_dir = f'{patch_coords_root}/{slide_name}' \n",
    "      candidates = os.listdir(target_dir)\n",
    "      target_file = candidates[0]\n",
    "      if not target_file[-3:] == 'npy':\n",
    "          print('file format error')\n",
    "      \n",
    "      coords = np.load(f'{target_dir}/{target_file}')\n",
    "      color_list = ['g', 'b']\n",
    "      for i, coord in enumerate(coords):\n",
    "          x = coord[0]\n",
    "          y = coord[1]\n",
    "          \n",
    "          pred_label = int(y_target_pred_binary_each[i])\n",
    "          color = color_list[pred_label]\n",
    "          \n",
    "          pred_prob = float(y_target_pred_prob_each[i])\n",
    "          alpha = pred_prob\n",
    "          if pred_label == 0:\n",
    "            alpha = 1 - alpha\n",
    "          \n",
    "          # pred binary : color # green : label_0 / blue : label_1\n",
    "          # pred prob : alpha\n",
    "          ax.add_patch(plt.Rectangle((x, y), patch_size//16, patch_size//16, edgecolor=color, facecolor=color, alpha=alpha))\n",
    "      plt.show()\n",
    "\n",
    "  print(f'cnt_correct : {cnt_correct}')\n",
    "  print(f'cnt_wrong : {cnt_wrong}')\n",
    "  accuracy = cnt_correct/(cnt_correct + cnt_wrong)\n",
    "  print(\"Slide Accuracy: {:.2f}%\".format(accuracy * 100))  # dataset의 slide 전체 종합 accuracy를 말하는 것!\n",
    "\n",
    "  if show_label_accuracy:\n",
    "    print(\"label_1 에 대한 정답률: {:.2f}%\".format(label_1_correct_cnt/label_1_cnt * 100))\n",
    "    print(\"label_0 에 대한 정답률: {:.2f}%\".format(label_0_correct_cnt/label_0_cnt * 100))\n",
    "\n",
    "  if use_threshold:\n",
    "    print(f'아래는 threshold {threshold*100}% 이상의 패치가 label 1일때 slide label을 1로 지정했을때의 결과')\n",
    "    print(f'cnt_correct : {t_cnt_correct}')\n",
    "    print(f'cnt_wrong : {t_cnt_wrong}')\n",
    "    accuracy = t_cnt_correct/(t_cnt_correct + t_cnt_wrong)\n",
    "    print(\"Slide Accuracy: {:.2f}%\".format(accuracy * 100))  \n",
    "\n",
    "    if show_label_accuracy:\n",
    "      print(\"label_1 에 대한 정답률: {:.2f}%\".format(t_label_1_correct_cnt/label_1_cnt * 100))\n",
    "      print(\"label_0 에 대한 정답률: {:.2f}%\".format(t_label_0_correct_cnt/label_0_cnt * 100))\n",
    "\n",
    "img_cnt = len(os.listdir(patch_samples_root))\n",
    "\n",
    "val_slide_list = val_dataset.get_target_slide_list()\n",
    "test_slide_list = test_dataset.get_target_slide_list()\n",
    "train_slide_list = dataset.get_target_slide_list()\n",
    "\n",
    "if show_slide_accuracy:\n",
    "    \n",
    "    slide_accuracy_test_on_target_set(val_slide_list, d_img, use_threshold=True, threshold=0.2)\n",
    "    slide_accuracy_test_on_target_set(test_slide_list, d_img, use_threshold=True, threshold=0.2)\n",
    "    slide_accuracy_test_on_target_set(train_slide_list, d_img, use_threshold=True, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if show_slide_pred_visualizer:\n",
    "  \n",
    "  dataset_name = 'val'\n",
    "\n",
    "  if dataset_name == 'val':\n",
    "    target_slide_list = val_slide_list\n",
    "  elif dataset_name == 'test':\n",
    "    target_slide_list = test_slide_list\n",
    "  elif dataset_name == 'train':\n",
    "    target_slide_list = train_slide_list\n",
    " \n",
    "  #target_slide_list = ['TCGA-B0-5115', 'TCGA-AK-3465']\n",
    "\n",
    "\n",
    "  #slide_accuracy_test_on_target_set(target_slide_list, d_img, use_threshold=True, threshold=0.2, show_visualizer=True, show_label_accuracy=True, show_label_only=1, is_cross_model_test=is_cross_model_test, dataset_name=dataset_name)\n",
    "  slide_accuracy_test_on_target_set(target_slide_list, d_img, use_threshold=True, threshold=0.2, show_visualizer=True, show_label_accuracy=True, show_label_only=None, is_cross_model_test=is_cross_model_test, dataset_name=dataset_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "print(model.layer4)\n",
    "\n",
    "def show_gradcam_img(target_slide_list, model):\n",
    "    model.eval()\n",
    "    slide_DSS_1_cnt = 0\n",
    "    \n",
    "    for slide_name in target_slide_list:\n",
    "        slide_dataset = CustomDataset(root = patch_samples_root, d_img=d_img, purpose='gradcam', slide_name=slide_name, show_detail=False)\n",
    "        dataloader = DataLoader(dataset=slide_dataset, batch_size=1, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        \n",
    "        if slide_DSS_1_cnt >= 8:\n",
    "            break \n",
    "\n",
    "    \n",
    "        (_, y_label) = dataloader.dataset[0]\n",
    "        y_label = int(y_label[0])\n",
    "        # check if the label is 1\n",
    "        if y_label == 1:\n",
    "            slide_DSS_1_cnt += 1\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        patch_idx = 0\n",
    "        true_positive_cnt_for_slide = 0\n",
    "        for X, Y in dataloader:\n",
    "            if true_positive_cnt_for_slide >= 3:\n",
    "                print(f'end of img {slide_name}')\n",
    "                break\n",
    "     \n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred_prob = model(X)\n",
    "\n",
    "            # Convert the predicted probabilities to binary predictions\n",
    "            y_pred_binary = torch.round(y_pred_prob)                \n",
    "        \n",
    "            img_name = f'{slide_name}_{patch_idx}'\n",
    "            patch_idx += 1\n",
    "\n",
    "            pred_prob = y_pred_prob[0]\n",
    "            pred = y_pred_binary[0]\n",
    "            pred = int(pred)\n",
    "            y_answer = Y[0]\n",
    "            y_answer = int(y_answer)\n",
    "\n",
    "            # check if the prediction is positive\n",
    "            if pred == 1:\n",
    "                true_positive_cnt_for_slide += 1\n",
    "                pass\n",
    "            else:\n",
    "                # 0에 대해서는 gradcam 적용 안되는 것 확인 (layer 활성화가 안되게 학습했을 것)\n",
    "                continue\n",
    "\n",
    "            pred_class_num = 0 # binary classification이라서 pred_class_num 0 밖에 못받음\n",
    "            targets = [ClassifierOutputTarget(pred_class_num)]\n",
    "            target_layers = [model.layer4[-1]]\n",
    "            input_tensor = X\n",
    "            img = X[0].permute((1,2,0))\n",
    "            img = img.to('cpu')\n",
    "            \n",
    "            plt.title(f'{img_name}/{np.round(float(pred_prob), 2)}')\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            # print(input_tensor.shape)\n",
    "            # print(img.shape)\n",
    "\n",
    "            # Construct the CAM object once, and then re-use it on many images:\n",
    "            cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "            # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            #print(grayscale_cam)\n",
    "\n",
    "            # In this example grayscale_cam has only one image in the batch:\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "            img = img.numpy()  # Convert the image tensor to a NumPy array\n",
    "            visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "            \n",
    "            plt.title(f'{img_name}/{np.round(float(pred_prob), 2)}')\n",
    "            plt.imshow(visualization)\n",
    "            plt.show()\n",
    "\n",
    "if show_gradcam:\n",
    "    # if is_year_sort == False:\n",
    "    #     img_cnt = len(os.listdir(patch_samples_root))\n",
    "    #     val_set_range = range(int(img_cnt*ratio_train), int(img_cnt*(ratio_train + ratio_val)))   \n",
    "    #     test_set_range = range(int(img_cnt*(ratio_train + ratio_val)), int(img_cnt))\n",
    "    #     train_set_range = range(0, int(img_cnt*(ratio_train)))\n",
    "    \n",
    "    gradcam_target_slide_list = val_slide_list\n",
    "    show_gradcam_img(gradcam_target_slide_list, model)\n",
    "\n",
    "\n",
    "\n",
    "    # print('show example image')\n",
    "    # img = np.load(f'{patch_samples_root}/TCGA-CJ-6033/TCGA-CJ-6033-1000.npy')\n",
    "    # img = img / 255\n",
    "    # img = img.astype(np.float32)\n",
    "\n",
    "    # print(img.shape)\n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "   \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import cm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def output_forward_before_fc_layer(model, x):\n",
    "  out = model.conv1(x)\n",
    "  out = model.bn1(out)\n",
    "  out = model.relu(out)\n",
    "  out = model.maxpool(out)\n",
    "  out = model.layer1(out)\n",
    "  out = model.layer2(out)\n",
    "  out = model.layer3(out)\n",
    "  out = model.layer4(out)\n",
    "  #print(out.shape) #torch.Size([batch_size, 512, 7, 7])\n",
    "  out_2 = model.avgpool(out)\n",
    "  out_2 = torch.squeeze(out_2)  \n",
    "  # print(out_2.shape) # torch.Size([500, 512, 1, 1]) => torch.Size([500, 512])\n",
    "  out_2 = model.fc(out_2)\n",
    "  return out, out_2\n",
    "\n",
    "\n",
    "class AnalyzeKernels:\n",
    "  def __init__(self, kernels, patch_size):\n",
    "    self.kernels = kernels\n",
    "    self.patch_size = patch_size\n",
    "    print(f'filter shape : {kernels.shape}')  \n",
    "\n",
    "  def show_kernels_with_weights(self):\n",
    "    # Calculate the magnitude of the weights for each kernel\n",
    "    kernels = self.kernels\n",
    "    weights_sum = torch.sum(torch.abs(kernels), dim=(1, 2, 3))\n",
    "    sorted_tensor, indices = torch.sort(weights_sum, descending=True)\n",
    "    kernels = kernels[indices]\n",
    "\n",
    "    plt.figure(figsize=(20, 17))\n",
    "    for i, kernel_img in enumerate(kernels):\n",
    "      if i > 8:\n",
    "        break\n",
    "      kernel_img = kernel_img.cpu().detach().numpy()\n",
    "      kernel_img = np.sum(kernel_img, axis=(0))\n",
    "      plt.subplot(8, 8, i+1).set_title(\"weight sum:{0:.4f}\".format(sorted_tensor[i].cpu().detach().numpy())) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "      plt.imshow(kernel_img, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "\n",
    "  def show_channels_of_nth_discriminative_kernel(self, nth=0):\n",
    "    \n",
    "    # Calculate the magnitude of the weights for each kernel\n",
    "    kernels = self.kernels\n",
    "    weights_sum = torch.sum(torch.abs(kernels), dim=(1, 2, 3))\n",
    "    sorted_tensor, indices = torch.sort(weights_sum, descending=True)\n",
    "    kernels = kernels[indices]\n",
    "\n",
    "    target_kernel = kernels[nth]\n",
    "    weights_sum = torch.sum(torch.abs(target_kernel), dim=(1, 2))\n",
    "    sorted_tensor, indices = torch.sort(weights_sum, descending=True)\n",
    "    target_kernel = target_kernel[indices]\n",
    "\n",
    "    plt.figure(figsize=(20, 17))\n",
    "    for i, channel in enumerate(target_kernel):\n",
    "      if i > 8:\n",
    "        break\n",
    "      kernel_img = channel.cpu().detach().numpy()\n",
    "      plt.subplot(8, 8, i+1).set_title(\"weight sum:{0:.4f}\".format(sorted_tensor[i].cpu().detach().numpy())) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "      plt.imshow(kernel_img, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  def get_entropy_of_filter(self, filter):\n",
    "    min_val = torch.min(filter)\n",
    "    filter = (filter - min_val) # 음수를 없애준다 / 전체를 위로 올리는 작업\n",
    "    sum = torch.sum(filter, axis=(0,1,2))\n",
    "    filter = filter / sum\n",
    "    prob_sum = torch.sum(filter, axis=(0,1,2))\n",
    "    #print(f'prob_sum : {prob_sum}')\n",
    "    \n",
    "    entropy = 0\n",
    "    for i in range(filter.shape[0]):\n",
    "      for j in range(filter.shape[1]):\n",
    "        for k in range(filter.shape[2]):\n",
    "          p = filter[i][j][k]\n",
    "          if p == 0:\n",
    "            continue\n",
    "          #print(p)\n",
    "          entropy += -p * torch.log(p)\n",
    "    return entropy\n",
    "\n",
    "  def get_entropy_of_filters(self, filters):\n",
    "    \n",
    "    file_name = f'filter_{PATH}.pt'\n",
    "    if os.path.exists(file_name):\n",
    "      entropy_list = torch.load(file_name)\n",
    "    else:  \n",
    "      entropy_list = torch.FloatTensor([self.get_entropy_of_filter(filter) for filter in filters])\n",
    "      torch.save(entropy_list, file_name)\n",
    "\n",
    "    print('sort by entropy complete')\n",
    "    sorted_tensor, indices = torch.sort(entropy_list, descending=True)\n",
    "    print(f'max entropy : {sorted_tensor[0]} min entropy : {sorted_tensor[-1]}')\n",
    "    return sorted_tensor, indices\n",
    "\n",
    "  def sort_and_show_kernels(self):\n",
    "    # Calculate the magnitude of the weights for each kernel\n",
    "    kernels = self.kernels\n",
    "    sorted_tensor, indices = self.get_entropy_of_filters(kernels)\n",
    "    self.filter_info = [sorted_tensor, indices]\n",
    "    weights_sum = torch.sum(torch.abs(kernels), dim=(1, 2, 3)) # 참고 정보\n",
    "    kernels = kernels[indices]\n",
    "\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(20, 17))\n",
    "    for i, kernel_img in enumerate(kernels):\n",
    "      if i > 8:\n",
    "        break\n",
    "      kernel_img = kernel_img.cpu().detach().numpy()\n",
    "      kernel_img = np.sum(kernel_img, axis=(0)) # - value가 검은색으로 지정되어서 기준을 새로잡아주게 됨\n",
    "      # print(kernel_img)\n",
    "      plt.subplot(8, 8, i+1).set_title(\"entropy:{0:.4f}\\n w:{1:.4f}\".format(sorted_tensor[i].cpu().detach().numpy(), weights_sum[indices[i]].cpu().detach().numpy())) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "      plt.imshow(kernel_img, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "\n",
    "  def twodim_embedding_visuallize_for_filtered_images(self, num_of_filters_to_show, model, patch_size=None, dataset=None, cnt=None, is_lda=False, is_use_entropy_metric=True, use_pred_label=False):\n",
    "\n",
    "    # filter setting\n",
    "    kernels = self.kernels\n",
    "    weights_sum = torch.sum(torch.abs(kernels), dim=(1, 2, 3)) # 참고 정보\n",
    "    #print(weights_sum)\n",
    "    \n",
    "    if is_use_entropy_metric:\n",
    "      filter_info = self.filter_info\n",
    "      sorted_tensor = filter_info[0]\n",
    "      indices = filter_info[1]\n",
    "      kernels = kernels[indices]  \n",
    "    else:\n",
    "      sorted_tensor, indices = torch.sort(weights_sum, descending=True)\n",
    "      kernels = kernels[indices]\n",
    "      \n",
    "    # filter setting end\n",
    "    batch_size = 500\n",
    "    loop_cnt = cnt // batch_size\n",
    "    if cnt % batch_size != 0:\n",
    "      print(f'please use multiples of {batch_size}.')\n",
    "      return\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "    # torch.cat((tensor1, tensor2), dim=0)\n",
    "\n",
    "    plt.figure(figsize=(20, 17))\n",
    "    for nth in range(num_of_filters_to_show):\n",
    "      nth_discriminative_kernel = kernels[nth]\n",
    "      kernel_img = nth_discriminative_kernel.cpu().detach().numpy()\n",
    "      kernel_img = np.sum(kernel_img, axis=(0))\n",
    "\n",
    "      final_dimension_of_output = 36\n",
    "      total_out_images = np.zeros((cnt, final_dimension_of_output))\n",
    "      total_labels = np.zeros((cnt, 1))\n",
    "      total_pred_labels = np.zeros((cnt, 1))\n",
    "      i = -1\n",
    "      for i, (xset, yset) in enumerate(data_loader):\n",
    "        i += 1\n",
    "        if i >= loop_cnt:\n",
    "          break\n",
    "\n",
    "        # cuDNN에러로 인해, continous한 메모리에 넣어줘야함\n",
    "        images = xset.to(device).contiguous()\n",
    "        labels = yset.to(device).contiguous()     \n",
    "        #print(f'images shape : {images.shape}')\n",
    "        with torch.no_grad():\n",
    "          images = images.to(device)\n",
    "          layer1_output, y_pred_prob = output_forward_before_fc_layer(model, images)\n",
    "          pred_labels = torch.round(y_pred_prob) \n",
    "          pred_labels = pred_labels.cpu().detach().numpy()\n",
    "  \n",
    "        # images.cpu().detach()\n",
    "        out_images = torch.nn.functional.conv2d(layer1_output, nth_discriminative_kernel.unsqueeze(0), padding=3, stride=2)\n",
    "        out_images = out_images.squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        if isinstance(labels, np.ndarray) == False:\n",
    "          labels = labels.cpu().detach().numpy()\n",
    "     \n",
    "        length = out_images.shape[1] ** 2\n",
    "        out_images = out_images.reshape(-1, length)\n",
    "        total_out_images[i*(batch_size):(i+1)*(batch_size)] = out_images\n",
    "        total_labels[i*(batch_size):(i+1)*(batch_size)] = labels\n",
    "        total_pred_labels[i*(batch_size):(i+1)*(batch_size)] = pred_labels\n",
    "        #print(out_images.shape)\n",
    "      print(total_out_images.shape)\n",
    "\n",
    "      out_images = total_out_images\n",
    "\n",
    "      # labels = total_pred_labels\n",
    "      # original_labels = total_labels\n",
    "      labels = total_labels \n",
    "      pred_labels = total_pred_labels\n",
    "\n",
    "      if is_lda:\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(out_images, labels)\n",
    "        X_transformed = lda.transform(out_images)\n",
    "        print(X_transformed.shape)\n",
    "      else:\n",
    "        pca = PCA(n_components=2, whiten=False) \n",
    "        pca.fit(out_images)\n",
    "        X_transformed = pca.fit_transform(out_images)\n",
    "        print(X_transformed.shape)\n",
    "      #print(weights_sum[indices[nth]])\n",
    "      plt.subplot(3, 3, nth+1).set_title(\"entropy:{0:.4f}, weight_sum:{1:.4f}\".format(\n",
    "        sorted_tensor[nth].cpu().detach().numpy(), weights_sum[indices[nth]].cpu().detach().numpy()\n",
    "      ))\n",
    "      for i, item in enumerate(X_transformed):\n",
    "        if is_lda:\n",
    "          if use_pred_label:\n",
    "            item = [item[0], item[0] + 2 * labels[i] + pred_labels[i]]\n",
    "          else:\n",
    "            item = [item[0], item[0] + labels[i]]\n",
    "            \n",
    "        if i > 3000:\n",
    "          break\n",
    "        if labels[i] == 0:\n",
    "          plt.plot(item[0],item[1], 'bo') \n",
    "        elif labels[i] == 1:\n",
    "          plt.plot(item[0],item[1],'ro', zorder=3) \n",
    "      plt.grid(True)\n",
    "      print('kernel processed')\n",
    "\n",
    "\n",
    "for i,item in enumerate(model.parameters()):\n",
    "  if i == 57:\n",
    "    print(f'filter shape : {item.shape}')\n",
    "    kernels = item.data\n",
    "\n",
    "\n",
    "\n",
    "if show_filter_analysis:\n",
    "  is_use_entropy_metric = True\n",
    "  AK = AnalyzeKernels(kernels, patch_size=patch_size)\n",
    "  \n",
    "  if is_use_entropy_metric:\n",
    "    AK.sort_and_show_kernels()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_filter_analysis:\n",
    "  is_lda = True\n",
    "  is_use_entropy_metric = True\n",
    "  \n",
    "  use_pred_label = True # 4줄 그어줌 (양끝 : 정답 / 중간 : 실제 레이블과 다르게 예측 (0을 1로, 1을 0으로))\n",
    "  cnt = 20000\n",
    "  AK.twodim_embedding_visuallize_for_filtered_images(num_of_filters_to_show=9, model=model, patch_size=patch_size, dataset=val_dataset, cnt=cnt, is_lda=is_lda, is_use_entropy_metric=is_use_entropy_metric, use_pred_label=use_pred_label)\n",
    "  AK.twodim_embedding_visuallize_for_filtered_images(num_of_filters_to_show=9, model=model, patch_size=patch_size, dataset=test_dataset, cnt=cnt, is_lda=is_lda, is_use_entropy_metric=is_use_entropy_metric, use_pred_label=use_pred_label)\n",
    "  AK.twodim_embedding_visuallize_for_filtered_images(num_of_filters_to_show=9, model=model, patch_size=patch_size, dataset=dataset, cnt=cnt, is_lda=is_lda, is_use_entropy_metric=is_use_entropy_metric, use_pred_label=use_pred_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
